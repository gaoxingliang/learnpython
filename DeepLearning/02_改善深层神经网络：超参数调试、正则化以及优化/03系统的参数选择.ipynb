{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各种超参数 如何进行系统的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 经验1: 使用随机值来搜索\n",
    "# 经验2: 从粗到细的搜索\n",
    "## 以前更多的是用的网格搜索\n",
    "## 但是在nn中我们用的更多的是使用随机化的值\n",
    "因为我们并不完全知道每个参数的重要性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何选择超参数的合理范围?\n",
    "更多的使用对数区间来搜索:\n",
    "![img](img/22.png)\n",
    "\n",
    "# Batch normalization\n",
    "训练单个模型时, 我们会normalize 输入 (也就是标准化).<br>\n",
    "对于神经网络我们需要对后续的$a[l]$都需要归一化来更快的训练$w[l+1]$和$b[l+1]$. 一般情况下我们都是normalize $z[l]$而不是$a[l]$.\n",
    "\n",
    "![img](img/23.png)\n",
    "有时候我们并不想我们的均值一直是0 方差是1 因为这会导致激活函数总是在一个范围. 所以我们使用另外的两个参数来更新最终的z值.<br>\n",
    "## batch normal 能加快NN的原因\n",
    "直觉上来说, normal后能够更加经受住数据的shifting.  层数越深的层越能避免被越早的层的改变所影响 (你可以想象第0层就是X examples).\n",
    "\n",
    "## batch norm的测试模式\n",
    "因为在测试模式 我们只有一个测试用例, 那么每一层的均值和方差如何计算?\n",
    "我们这里采用的是原来训练层的均值和方差来计算的值(一般用指数加权平均来最终训练集中的均值和方差.)\n",
    "![img](img/24.png)\n",
    "\n",
    "# SoftMax \n",
    "用来解决多分类问题.<br>\n",
    "最后一层隐藏层的数量与分类数一致来告诉我们每一个分类的概率.\n",
    "\n",
    "![img](img/25.png)\n",
    "## softmax layer\n",
    "这一层的激活函数如下(假设有4类, 也就是最后一层的nn个数是4):\n",
    "![img](img/26.png)\n",
    "![img](img/28.png)  \n",
    "\n",
    "没有隐藏层时,它的决策边界是线性的\n",
    "![img](img/27.png)\n",
    "但是如果我们增加隐藏层就会有非线性的分类边界.\n",
    "HardMax -> 将a变成非0即1的2种选择.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
