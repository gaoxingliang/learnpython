{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inputlayers\n",
    "![nn11](img/nn11.png)\n",
    "单个圆圈计算了什么?\n",
    "![nn12](img/nn12.png)\n",
    "\n",
    "![nn12](img/nn13.png)\n",
    "\n",
    "# 多个训练数据时的向量化计算\n",
    "![nn12](img/nn14.png)\n",
    "![nn12](img/nn15.png)\n",
    "\n",
    "# 不同的激活函数\n",
    "## sigmoid\n",
    "## tanh 它一般更好因为它的数据均值在0  但是在output layer我们还是用sigmoid\n",
    "## 因为它保证了数据在0-1之间\n",
    "![nn16](img/nn16.png)\n",
    "sigmoid&tanh的缺点就是z很大时, z的斜率就非常小了(接近0,这个会降低梯度下降的速度), 所以我们有时候会用RELU\n",
    "\n",
    "## ReLU  这个一般在隐藏层作为默认的activefunction 在输出层还是使用sigmoid\n",
    "\n",
    "# 为什么需要非线性的激活函数\n",
    "如果不是这样的话<br>\n",
    "我们最终的输出就是输入的线性组合了, 那么不如直接去掉隐藏层直接计算<br>\n",
    "![nn16](img/nn17.png)\n",
    "## 除非是在回归问题中我们才可能会用到线性激活的函数作为输出层比如房价预测\n",
    "\n",
    "# 激活函数的导数\n",
    "sigmoid的导数:\n",
    "$$\n",
    "ds/dz = sigmoid * (1 - sigmoid)\n",
    "$$\n",
    "\n",
    "tanh的导数:\n",
    "$$\n",
    "dt/dz = 1-tanh^2\n",
    "$$\n",
    "RELU就是1个为0 一个为1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
