{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inputlayers\n",
    "![nn11](img/nn11.png)\n",
    "单个圆圈计算了什么?\n",
    "![nn12](img/nn12.png)\n",
    "\n",
    "![nn12](img/nn13.png)\n",
    "\n",
    "# 多个训练数据时的向量化计算\n",
    "![nn12](img/nn14.png)\n",
    "![nn12](img/nn15.png)\n",
    "\n",
    "# 不同的激活函数\n",
    "## sigmoid\n",
    "## tanh 它一般更好因为它的数据均值在0  但是在output layer我们还是用sigmoid\n",
    "## 因为它保证了数据在0-1之间\n",
    "![nn16](img/nn16.png)\n",
    "sigmoid&tanh的缺点就是z很大时, z的斜率就非常小了(接近0,这个会降低梯度下降的速度), 所以我们有时候会用RELU\n",
    "\n",
    "## ReLU  这个一般在隐藏层作为默认的activefunction 在输出层还是使用sigmoid\n",
    "\n",
    "# 为什么需要非线性的激活函数\n",
    "如果不是这样的话<br>\n",
    "我们最终的输出就是输入的线性组合了, 那么不如直接去掉隐藏层直接计算<br>\n",
    "![nn16](img/nn17.png)\n",
    "## 除非是在回归问题中我们才可能会用到线性激活的函数作为输出层比如房价预测\n",
    "\n",
    "# 激活函数的导数\n",
    "sigmoid的导数:\n",
    "$$\n",
    "ds/dz = sigmoid * (1 - sigmoid)\n",
    "$$\n",
    "\n",
    "tanh的导数:\n",
    "$$\n",
    "dt/dz = 1-tanh^2\n",
    "$$\n",
    "RELU就是1个为0 一个为1.\n",
    "\n",
    "# 神经网络中的梯度下降方法\n",
    "## 已知参数\n",
    "![nn17](img/nn18.png)\n",
    "## 以及损失函数\n",
    "![nn17](img/19.png)\n",
    "## 需要的等式\n",
    "![20](img/20.png)\n",
    "### 前向传播\n",
    "前向传播不用说了就是按照神经网络的计算图来的。\n",
    "\n",
    "### 反向传播\n",
    "公式1，2，3都是跟逻辑回归的方式一致的， 因为最后的输出层我们假设为sigmoid函数（参考原视频2.9节）：\n",
    "![20](img/22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 更加直观的关于NN的梯度计算的理解\n",
    "# 实际上是根据链式求导法则而已\n",
    "# 逻辑回归的就是上面的这个图\n",
    "# NN就是有2次求导而已\n",
    "![23](img/23.png)\n",
    "\n",
    "逻辑回归中的w是一个行向量，而我们这里NN的W是一个列向量。 所以有一个多的转置操作。\n",
    "![23](img/24.png)\n",
    "![23](img/25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么逻辑回归中初始化为0（w,b）可以\n",
    "## 但是NN中不行\n",
    "如果对NN初始化为0，会导致中间的隐藏层实际上是同一个Node， 它们是对称的，\n",
    "这回导致多个神经元实际一个就够了。\n",
    "## 所以NN需要随机化初始化。\n",
    "一般是：w = np.random.randn((2,2)) * 0.01初始化为一个较小的数。\n",
    "b = np.zeros((2,1))\n",
    "乘以0.01避免我们的wx+b结果在斜率接近为0的位置。\n",
    "\n",
    "## 逻辑回归中是根据导数来更新的 不会存在对称性的情况"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
