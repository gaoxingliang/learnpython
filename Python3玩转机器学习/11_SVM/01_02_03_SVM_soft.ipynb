{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svm](img/svm1.png)\n",
    "![svm](img/svm2.png)\n",
    "有Hard margin svm 和 soft margin svm\n",
    "## 我们需要数学化如何最大化margin\n",
    "# SVM基础部分.\n",
    "对于如下的分类问题, 可以看到我们有很多分类的直线:\n",
    "![svm-01.png](img/svm-01.png)\n",
    "![svm-02.png](img/svm-02.png)\n",
    "而我们想要得到最好的分类结果, 那么我们可以需要让离分类边界最近的点离分类边界有最大的距离, 就像下面这个样子:\n",
    "\n",
    "![svm-03.png](img/svm-03.png)\n",
    "我们想要的就是margin 最大 也就是d 最大.\n",
    "\n",
    "回想以前的点(x,y)到直线Ax+By+C的距离公式:\n",
    "$$\\frac{|Ax+By+C|}{\\sqrt[2]{A^2+B^2}}$$\n",
    "\n",
    "### 这一小节我们都没有自动填充$x_0$\n",
    "以前的\n",
    "$$\n",
    "\\theta^\\mathrm Tx_b = 0\n",
    "$$\n",
    "\n",
    "那么类似的对分类直线(b就是最后的截距, w是n*1的向量):\n",
    "$$w^Tx+b=0$$\n",
    "可以看到分母跟b是没有关系的. 那么任意一个点到该分类直线的距离是:\n",
    "$$\\frac{|w^T+b|}{||w||}$$\n",
    "其中:\n",
    "$$||w||=\\sqrt[2]{w_1^2 + w_2^2 + .... + w_n^n}$$\n",
    "\n",
    "那么对于分类正确的数据(hard margin SVM要求如此):\n",
    "这里y的取值并不重要 重要的是2个不同的值可以让2分类分开:\n",
    "\n",
    "(1) 对$y^{(i)}=1$有:\n",
    "$$\\frac{w^Tx^{(i)} + b}{||w||}\\ge d$$\n",
    "\n",
    "(2) 对于$y^{(i)}=-1$有:\n",
    "$$\\frac{w^Tx^{(i)} + b}{||w||}\\le -d$$\n",
    "\n",
    "那么我们两边同时除d的话,切换为:\n",
    "\n",
    "(1) 对于$y^{(i)}=1$有:\n",
    "$$\\frac{w^Tx^{(i)} + b}{||w||d}\\ge 1$$\n",
    "\n",
    "(2) 对于$y^{(i)}=-1$有:\n",
    "$$\\frac{w^Tx^{(i)} + b}{||w||d}\\le -1$$\n",
    "\n",
    "换一个记号:\n",
    "(1) 对于$y^{(i)}=1$有:\n",
    "$$w_d^Tx^{(i)} + b_d\\ge 1$$\n",
    "\n",
    "(2) 对于$y^{(i)}=-1$有:\n",
    "$$w_d^Tx^{(i)} + b_d\\le -1$$\n",
    "\n",
    "\n",
    "将$w_d$直接替换为w, 那么这两个不等式可以合为1个:\n",
    "$$y^{(i)}(w^Tx^{(i)} + b)\\ge 1$$\n",
    "\n",
    "对于任意支撑向量x:\n",
    "$$max\\frac{|w^Tx^{(i)} + b|}{||w||}$$\n",
    "因为$w^Tx+b = 1$ 那么也就是$max\\frac{1}{||w||}$, \n",
    "\n",
    "SVM结论:\n",
    "<b>再替换下就是<font color=\"red\">SVM的最优化目标就是</font></b> \n",
    "$$min\\frac{1}{2}||w||^2$$\n",
    "s.t.\n",
    "$$y^{(i)}(w^Tx^{(i)} + b)\\ge 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 有条件的最优化问题\n",
    "一般无条件的都是用求导来解决.(全局最优化问题, 比如线性回归 逻辑回归, 一般都是求导, 或者梯度下降的办法就可以了)\n",
    "\n",
    "而有条件问题的都是使用$\\lambda$算子解决.\n",
    "\n",
    "# soft margin SVM\n",
    "前面的hard marin svm 要求太过严格, 必须要求线性可分(可能是直线, 也可能是超平面的一个分割)\n",
    "对于如下的例子, 也许有错误的数据或者outlier, 很明显如果采用hard svm 可以看到我们模型的泛化能力有可能很弱:\n",
    "![svm-04.png](img/svm-04.png)\n",
    "或者像下面的这样,根本不可分(如果我们对hard margin svm 不做任何容错处理的话我们就无法工作了):\n",
    "![svm-05.png](img/svm-05.png)\n",
    "\n",
    "那么我们宽松下条件, 就是 <font color=red>soft margin svm</font> \n",
    "$$min\\frac{1}{2}||w||^2$$\n",
    "s.t.\n",
    "$$y^{(i)}(w^Tx^{(i)} + b)\\ge 1 - \\zeta_i  $$  ($\\zeta_i\\ge 0$)\n",
    "![svm-06.png](img/svm-06.png)\n",
    "注意到:$\\zeta_i$是对每个数据都有一个值, 如果这个值很大的话, 可想而知,容错率范围大大了.\n",
    "那么我们用如下的方式来表达不能让容错率太大\n",
    "(如果我们有m个数据的话):\n",
    "$$min\\frac{1}{2}||w||^2 + \\sum_{i=1}^{m}{\\zeta_i}$$\n",
    "\n",
    "如果我们需要平衡w和容错率之间的关系(加入新的超参数C, C 越大, 那么需要$\\zeta$越小, 那么就会退化为Hard Margin, 也叫<font color=red>L1正则(提高泛化能力)</font>):\n",
    "$$min\\frac{1}{2}||w||^2 + C\\sum_{i=1}^{m}{\\zeta_i}$$\n",
    "\n",
    "可想而知L2正则就是:\n",
    "$$min\\frac{1}{2}||w||^2 + C\\sum_{i=1}^{m}{\\zeta_i^2}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
